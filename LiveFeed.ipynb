{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __draw_label(img, text, pos, bg_color):\n",
    "   font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "   scale = 1\n",
    "   color = (0, 0, 0)\n",
    "   thickness = cv2.FILLED\n",
    "   margin = 2\n",
    "   txt_size = cv2.getTextSize(text, font_face, scale, thickness)\n",
    "\n",
    "   end_x = pos[0] + txt_size[0][0] + margin\n",
    "   end_y = pos[1] - txt_size[0][1] - margin\n",
    "\n",
    "   cv2.rectangle(img, pos, (end_x, end_y), bg_color, thickness)\n",
    "   cv2.putText(img, text, pos, font_face, scale, color, 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_Input(X):\n",
    "    np_X = np.array(X)\n",
    "    normalised_X = np_X.astype('float32')/255.0\n",
    "    normalised_X = np.expand_dims(normalised_X, axis=0)\n",
    "    return normalised_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">If you want to load and use mobile net TFLite model \n",
    "Run the next 2 cells</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path='ASL_words_mobilenet_v2_130_224.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', \n",
    "           'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', \n",
    "           'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Check if camera was opened correctly\n",
    "if not (cap.isOpened()):\n",
    "    print(\"Could not open video device\")\n",
    "\n",
    "# 2) fetch one frame at a time from your camera\n",
    "while(True):\n",
    "    \n",
    "    # frame is a numpy array, that you can predict on \n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # 3) obtain the prediction\n",
    "    # depending on your model, you may have to reshape frame\n",
    "    frame1 = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    try:\n",
    "        image_pixels = tf.keras.utils.img_to_array(frame1)\n",
    "        image_pixels = preprocess_Input(image_pixels)\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_pixels)\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get the result \n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predicted_index = np.argmax(output_data)\n",
    "        print(predicted_index)\n",
    "\n",
    "        print(\"Predicted label: \" + classes[predicted_index])\n",
    "        res = classes[predicted_index]\n",
    "        #sequence = categories[mx][1]\n",
    "    except:\n",
    "        continue \n",
    "    # you may need then to process prediction to obtain a label of your data, depending on your model. Probably you'll have to apply an argmax to prediction to obtain a label.\n",
    "    \n",
    "    # 4) Adding the label on your frame\n",
    "    __draw_label(frame, res, (20,20), (255,0,0))\n",
    "\n",
    "\n",
    "    # 5) Display the resulting frame\n",
    "    cv2.imshow(\"preview\",frame)\n",
    "    #Waits for a user input to quit the application\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">If you want to load and use mobile net Model \n",
    "Run the next 2 cells</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Keras Model\n",
    "model = tf.keras.models.load_model('ASL_words.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', \n",
    "           'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', \n",
    "           'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 159ms/step\n",
      "1175\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1175\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1233\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1233\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "160\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1233\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "827\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1233\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1233\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "363\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1233\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1233\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "363\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1030\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "15\n",
      "Predicted label: nothing\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "363\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1233\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1233\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "624\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "566\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1030\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "566\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1233\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "566\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "769\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "566\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1175\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "363\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "421\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1030\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "160\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "769\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "566\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "566\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "363\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "566\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "160\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "160\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1030\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1030\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "566\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1175\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1175\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "421\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1175\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1146\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1175\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1175\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "972\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "15\n",
      "Predicted label: nothing\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Check if camera was opened correctly\n",
    "if not (cap.isOpened()):\n",
    "    print(\"Could not open video device\")\n",
    "\n",
    "# 2) fetch one frame at a time from your camera\n",
    "while(True):\n",
    "    \n",
    "    # frame is a numpy array, that you can predict on \n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # 3) obtain the prediction\n",
    "    # depending on your model, you may have to reshape frame\n",
    "    frame1 = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    try:\n",
    "        image_pixels = tf.keras.utils.img_to_array(frame1)\n",
    "        #image_pixels = np.expand_dims(image_pixels, axis = 0)\n",
    "        image_pixels = preprocess_Input(image_pixels)\n",
    "\n",
    "        # Get the result \n",
    "        output_data = model.predict(image_pixels)\n",
    "        predicted_index = np.argmax(output_data)\n",
    "        print(predicted_index)\n",
    "\n",
    "        print(\"Predicted label: \" + classes[predicted_index])\n",
    "        res = classes[predicted_index]\n",
    "        #sequence = categories[mx][1]\n",
    "    except:\n",
    "        continue \n",
    "    # you may need then to process prediction to obtain a label of your data, depending on your model. Probably you'll have to apply an argmax to prediction to obtain a label.\n",
    "    \n",
    "    # 4) Adding the label on your frame\n",
    "    __draw_label(frame, res, (20,20), (255,0,0))\n",
    "\n",
    "\n",
    "    # 5) Display the resulting frame\n",
    "    cv2.imshow(\"preview\",frame)\n",
    "    #Waits for a user input to quit the application\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">If you want to load and use ASL_Kaggle Model \n",
    "Run the next 2 cells</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Keras Model\n",
    "model = tf.keras.models.load_model('words_model_kaggle.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "classes = ['A', 'B', 'C', 'D','del', 'E', 'F', 'G', 'H', 'I', 'J', 'K', \n",
    "           'L', 'M', 'N', 'nothing', 'O', 'P', 'Q', 'R', 'S', 'space', 'T', 'U', 'V', \n",
    "           'W', 'X', 'Y', 'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 134ms/step\n",
      "nothing\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "nothing\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "nothing\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "nothing\n"
     ]
    }
   ],
   "source": [
    "# Open the device at the ID 0\n",
    "# Use the camera ID based on\n",
    "# /dev/videoID needed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Check if camera was opened correctly\n",
    "if not (cap.isOpened()):\n",
    "    print(\"Could not open video device\")\n",
    "\n",
    "# 2) fetch one frame at a time from your camera\n",
    "while(True):\n",
    "    \n",
    "    # frame is a numpy array, that you can predict on \n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # 3) obtain the prediction\n",
    "    # depending on your model, you may have to reshape frame\n",
    "    frame1 = cv2.resize(frame, (32, 32), interpolation=cv2.INTER_CUBIC)\n",
    "    frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    try:\n",
    "        image_pixels = tf.keras.utils.img_to_array(frame1)\n",
    "        #image_pixels = np.expand_dims(image_pixels, axis = 0)\n",
    "        image_pixels = preprocess_Input(image_pixels)\n",
    "        proba = model.predict(image_pixels)\n",
    "        mx = np.argmax(proba)\n",
    "\n",
    "        res = classes[mx]\n",
    "        print(res)\n",
    "        #sequence = categories[mx][1]\n",
    "    except:\n",
    "        continue \n",
    "    # you may need then to process prediction to obtain a label of your data, depending on your model. Probably you'll have to apply an argmax to prediction to obtain a label.\n",
    "    \n",
    "    # 4) Adding the label on your frame\n",
    "    __draw_label(frame, res, (20,20), (255,0,0))\n",
    "\n",
    "\n",
    "    # 5) Display the resulting frame\n",
    "    cv2.imshow(\"preview\",frame)\n",
    "   \n",
    "    #Waits for a user input to quit the application\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('GPEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cacc230392650fa4286313c795167cdc364f7039153613550d3da1c389fd26be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
